{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Quantitized Model Benchmarking Efficiency in Feed-Forward Neural Networks.\n",
    "\n",
    "**Team Cyclops**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Below is the code for building the network\n",
    "\n",
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "\n",
    "trainPath = \"./MNIST/mnist_train.csv\"\n",
    "testPath = \"./MNIST/mnist_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Dataloader\n",
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, fileName: str, transform = None):\n",
    "        self.X_Data, self.Y_Data = [], []\n",
    "\n",
    "        with open(fileName, mode = 'r') as f:\n",
    "\n",
    "            csvreader = csv.reader(f)\n",
    "\n",
    "            for line in csvreader:\n",
    "                row = list(map(int, line))\n",
    "\n",
    "                pixels = row[1:]\n",
    "                label = row[0]\n",
    "\n",
    "                pixels = (pixels - np.mean(pixels, axis = 0))/np.std(pixels, axis = 0)\n",
    "\n",
    "                self.X_Data.append(pixels)\n",
    "                self.Y_Data.append(label)\n",
    "\n",
    "        self.transform = transform\n",
    "        self.length = len(self.Y_Data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        X_tensor = torch.FloatTensor(self.X_Data[idx])\n",
    "        Y_tensor = torch.tensor(self.Y_Data[idx])\n",
    "\n",
    "        if self.transform:\n",
    "            X_tensor = self.transform(X_tensor)\n",
    "\n",
    "        return X_tensor, Y_tensor\n",
    "    \n",
    "\n",
    "\n",
    "trainMNIST  = MNISTDataset(trainPath, None)\n",
    "testMNIST = MNISTDataset(testPath, None)\n",
    "\n",
    "train_loader_temp = torch.utils.data.DataLoader(\n",
    "    dataset = trainMNIST,\n",
    "    batch_size = 64,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_loader_temp = torch.utils.data.DataLoader(\n",
    "    dataset = testMNIST,\n",
    "    batch_size = 64,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "\n",
    "r, c = [2, 2]\n",
    "fig, ax = plt.subplots(r, c, figsize= (8, 8))\n",
    "\n",
    "k = 0\n",
    "\n",
    "for data in train_loader_temp:\n",
    "    x, y = data\n",
    "    \n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            img = x[k].numpy().reshape(28, 28)\n",
    "            ax[i, j].imshow(img)\n",
    "            ax[i, j].axis('off')\n",
    "            k += 1\n",
    "\n",
    "    break\n",
    "\n",
    "del train_loader_temp, test_loader_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLoop(inputModel, inputOptimizer, inputCriterion, dataloader, num_epochs = 2):\n",
    "\n",
    "    startTime = time.time()\n",
    "\n",
    "    # Looping over the epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Putting the model in training loop\n",
    "        inputModel.train()\n",
    "\n",
    "        runningLoss = 0\n",
    "\n",
    "        for pixels, labels in dataloader:\n",
    "\n",
    "            # Zeroing the gradients\n",
    "            inputOptimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = inputModel(pixels)\n",
    "            loss = inputCriterion(outputs, labels)\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            inputOptimizer.step()\n",
    "\n",
    "            runningLoss += loss.item()\n",
    "\n",
    "        epochLoss = runningLoss/len(dataloader)\n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {epochLoss:.4f}')\n",
    "    endTime = time.time()\n",
    "    return (endTime - startTime)/num_epochs\n",
    "\n",
    "\n",
    "def testLoop(inputModel, dataloader, batch_size):\n",
    "\n",
    "    startTime = time.time()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predicitions = 0\n",
    "\n",
    "    # Putting the model in evaluation mode\n",
    "    inputModel.eval()\n",
    "\n",
    "    # Ensuring no gradients are calculated\n",
    "    with torch.no_grad():\n",
    "        for pixels, numbers in dataloader:\n",
    "            \n",
    "            # Forward Pass\n",
    "            outputs = inputModel(pixels)\n",
    "\n",
    "            # Inference \n",
    "            correct_predictions += torch.sum(torch.argmax(outputs, dim = 1) == numbers).item()\n",
    "            total_predicitions += len(numbers)\n",
    "\n",
    "\n",
    "    accuracy = 100 * correct_predictions/total_predicitions\n",
    "    endTime = time.time()\n",
    "    return accuracy, (endTime - startTime)/(len(dataloader)*batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Computing the Parameters in the model\n",
    "def getParams(inputModel):\n",
    "    \n",
    "    model_size = 0\n",
    "    for f in inputModel.parameters():\n",
    "        if f.requires_grad:\n",
    "\n",
    "            model_size += f.numel()\n",
    "\n",
    "    return model_size\n",
    "\n",
    "# Computing FLOPS\n",
    "def getFLOPS(inputModel):\n",
    "\n",
    "    flops = 0\n",
    "\n",
    "    for f in inputModel.parameters():\n",
    "        if f.requires_grad:\n",
    "\n",
    "            if f.ndim == 1:\n",
    "                flops += 2*f.shape[0]\n",
    "            else:\n",
    "                flops += 2*f.shape[0]*f.shape[1]\n",
    "\n",
    "    return flops\n",
    "\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "    os.remove('temp.p')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = trainMNIST,\n",
    "    batch_size = 64,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_loader_1 = torch.utils.data.DataLoader(\n",
    "    dataset = testMNIST,\n",
    "    batch_size = 1,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_loader_64 = torch.utils.data.DataLoader(\n",
    "    dataset = testMNIST,\n",
    "    batch_size = 64,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "# BaseLine Model\n",
    "class BaselineMLP(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size = 784, output_size = 10, hidden_size = 1024):\n",
    "\n",
    "        super(BaselineMLP, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time per epoch:  7.873818516731262\n",
      "Model precision: fp32\n",
      "model:    \t Size (MB): 7.456639\n",
      "Number of parameters: 1863690\n"
     ]
    }
   ],
   "source": [
    "baseline_model  = BaselineMLP(input_size = 784, output_size = 10, hidden_size = 1024).to(device)\n",
    "baseline_criterion = torch.nn.CrossEntropyLoss()\n",
    "baseline_optimizer = torch.optim.Adam(baseline_model.parameters(), lr = 0.001)\n",
    "\n",
    "predTime = trainLoop(inputModel = baseline_model, inputOptimizer = baseline_optimizer, inputCriterion = baseline_criterion, dataloader = train_loader, num_epochs = 2)\n",
    "print(\"Training time per epoch: \", predTime)\n",
    "\n",
    "print(\"Model precision: fp32\")\n",
    "model_size_storgae = print_size_of_model(baseline_model)\n",
    "\n",
    "model_size_params = getParams(baseline_model)\n",
    "print(\"Number of parameters: {}\".format(model_size_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run- 1\n",
      "Inference Time:  0.00014962770938873292\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "Run- 2\n",
      "Inference Time:  0.00012966232299804688\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "Run- 3\n",
      "Inference Time:  0.0001537252902984619\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "Run- 4\n",
      "Inference Time:  0.00012429850101470946\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "Run- 5\n",
      "Inference Time:  0.0001420781135559082\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "\n",
      "Average Accuracy: 97.37\n",
      "Average Inference Time: 0.00013987838745117186\n",
      "Std Inference Time: 1.1302438904270402e-05\n"
     ]
    }
   ],
   "source": [
    "# Batch Size 1 experiments\n",
    "all_accuracies_1, all_inference_1 = [], []\n",
    "for i in range(5):\n",
    "    print(\"\\nRun-\", (i + 1))\n",
    "    baselineAcc, baselineInf = testLoop(inputModel = baseline_model, dataloader = test_loader_1, batch_size = 1)\n",
    "\n",
    "    all_accuracies_1.append(baselineAcc)\n",
    "    all_inference_1.append(baselineInf)\n",
    "    \n",
    "    print(\"Inference Time: \", baselineInf)\n",
    "    print(\"Testing Accuracy: \", baselineAcc)\n",
    "\n",
    "\n",
    "print(\"\\n\\nAverage Accuracy: {}\".format(np.array(all_accuracies_1).mean()))\n",
    "print(\"Average Inference Time: {}\".format(np.array(all_inference_1).mean()))\n",
    "print(\"Std Inference Time: {}\".format(np.array(all_inference_1).std ()))\n",
    "\n",
    "del all_accuracies_1, all_inference_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run- 1\n",
      "Inference Time:  2.4155565318028638e-05\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "Run- 2\n",
      "Inference Time:  1.6190182821006532e-05\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "Run- 3\n",
      "Inference Time:  1.7612152228689496e-05\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "Run- 4\n",
      "Inference Time:  1.804312323309054e-05\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "Run- 5\n",
      "Inference Time:  2.2014614882742524e-05\n",
      "Testing Accuracy:  97.37\n",
      "\n",
      "\n",
      "Average Accuracy: 97.37\n",
      "Average Inference Time: 1.9603127696711548e-05\n",
      "Std Inference Time: 2.9861596979987197e-06\n"
     ]
    }
   ],
   "source": [
    "# Batch Size 64 experiments\n",
    "all_accuracies_64, all_inference_64 = [], []\n",
    "for i in range(5):\n",
    "    print(\"\\nRun-\", (i + 1))\n",
    "    baselineAcc, baselineInf = testLoop(inputModel = baseline_model, dataloader = test_loader_64, batch_size = 64)\n",
    "\n",
    "    all_accuracies_64.append(baselineAcc)\n",
    "    all_inference_64.append(baselineInf)\n",
    "    \n",
    "    print(\"Inference Time: \", baselineInf)\n",
    "    print(\"Testing Accuracy: \", baselineAcc)\n",
    "\n",
    "print(\"\\n\\nAverage Accuracy: {}\".format(np.array(all_accuracies_64).mean()))\n",
    "print(\"Average Inference Time: {}\".format(np.array(all_inference_64).mean()))\n",
    "print(\"Std Inference Time: {}\".format(np.array(all_inference_64).std ()))\n",
    "\n",
    "del all_accuracies_64, all_inference_64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic quantization in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization.quantize_fx import prepare_fx, prepare_qat_fx, convert_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Float 16 Quantization (Will not run on Mac M1, use Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.quantized.engine = 'fbgemm'\n",
    "# model_dynamic_float_16 = torch.quantization.quantize_dynamic(baseline_model, {torch.nn.Linear}, dtype=torch.float16)\n",
    "\n",
    "# print(\"Model precision: float16\")\n",
    "# model_size_storgae = print_size_of_model(model_dynamic_float_16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batch Size 1 experiments\n",
    "# all_accuracies_float16_1, all_inference_float16_1 = [], []\n",
    "# for i in range(5):\n",
    "#     print(\"\\nRun-\", (i + 1))\n",
    "#     baselineAcc, baselineInf = testLoop(inputModel = model_dynamic_float_16, dataloader = test_loader_1, batch_size = 1)\n",
    "\n",
    "#     all_accuracies_float16_1.append(baselineAcc)\n",
    "#     all_inference_float16_1.append(baselineInf)\n",
    "    \n",
    "#     print(\"Inference Time: \", baselineInf)\n",
    "#     print(\"Testing Accuracy: \", baselineAcc)\n",
    "\n",
    "\n",
    "# print(\"\\n\\nAverage Accuracy: {}\".format(np.array(all_accuracies_float16_1).mean()))\n",
    "# print(\"Average Inference Time: {}\".format(np.array(all_inference_float16_1).mean()))\n",
    "# print(\"Std Inference Time: {}\".format(np.array(all_inference_float16_1).std ()))\n",
    "\n",
    "# del all_accuracies_float16_1, all_inference_float16_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Batch Size 64 experiments\n",
    "# all_accuracies_float16_64, all_inference_float16_64 = [], []\n",
    "# for i in range(5):\n",
    "#     print(\"\\nRun-\", (i + 1))\n",
    "#     baselineAcc, baselineInf = testLoop(inputModel = model_dynamic_float_16, dataloader = test_loader_64, batch_size = 64)\n",
    "\n",
    "#     all_accuracies_float16_64.append(baselineAcc)\n",
    "#     all_inference_float16_64.append(baselineInf)\n",
    "    \n",
    "#     print(\"Inference Time: \", baselineInf)\n",
    "#     print(\"Testing Accuracy: \", baselineAcc)\n",
    "\n",
    "# print(\"\\n\\nAverage Accuracy: {}\".format(np.array(all_accuracies_float16_64).mean()))\n",
    "# print(\"Average Inference Time: {}\".format(np.array(all_inference_float16_64).mean()))\n",
    "# print(\"Std Inference Time: {}\".format(np.array(all_inference_float16_64).std ()))\n",
    "\n",
    "# del all_accuracies_float16_64, all_inference_float16_64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q INT8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model precision: qint8\n",
      "model:    \t Size (MB): 1.873623\n"
     ]
    }
   ],
   "source": [
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "model_dynamic_q_int8 = torch.quantization.quantize_dynamic(baseline_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "print(\"Model precision: qint8\")\n",
    "model_size_storgae = print_size_of_model(model_dynamic_q_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run- 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W qlinear_dynamic.cpp:247] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time:  0.0005500967979431153\n",
      "Testing Accuracy:  97.4\n",
      "\n",
      "Run- 2\n",
      "Inference Time:  0.00040484278202056884\n",
      "Testing Accuracy:  97.4\n",
      "\n",
      "Run- 3\n",
      "Inference Time:  0.00033933110237121584\n",
      "Testing Accuracy:  97.4\n",
      "\n",
      "Run- 4\n",
      "Inference Time:  0.0003362049102783203\n",
      "Testing Accuracy:  97.4\n",
      "\n",
      "Run- 5\n",
      "Inference Time:  0.00034634108543395994\n",
      "Testing Accuracy:  97.4\n",
      "\n",
      "\n",
      "Average Accuracy: 97.4\n",
      "Average Inference Time: 0.00039536333560943605\n",
      "Std Inference Time: 8.13324237086491e-05\n"
     ]
    }
   ],
   "source": [
    "# Batch Size 1 experiments\n",
    "all_accuracies_qint8_1, all_inference_qint8_1 = [], []\n",
    "for i in range(5):\n",
    "    print(\"\\nRun-\", (i + 1))\n",
    "    baselineAcc, baselineInf = testLoop(inputModel = model_dynamic_q_int8, dataloader = test_loader_1, batch_size = 1)\n",
    "\n",
    "    all_accuracies_qint8_1.append(baselineAcc)\n",
    "    all_inference_qint8_1.append(baselineInf)\n",
    "    \n",
    "    print(\"Inference Time: \", baselineInf)\n",
    "    print(\"Testing Accuracy: \", baselineAcc)\n",
    "\n",
    "\n",
    "print(\"\\n\\nAverage Accuracy: {}\".format(np.array(all_accuracies_qint8_1).mean()))\n",
    "print(\"Average Inference Time: {}\".format(np.array(all_inference_qint8_1).mean()))\n",
    "print(\"Std Inference Time: {}\".format(np.array(all_inference_qint8_1).std ()))\n",
    "\n",
    "del all_accuracies_qint8_1, all_inference_qint8_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run- 1\n",
      "Inference Time:  3.164479876779447e-05\n",
      "Testing Accuracy:  97.41\n",
      "\n",
      "Run- 2\n",
      "Inference Time:  2.558489039445379e-05\n",
      "Testing Accuracy:  97.42\n",
      "\n",
      "Run- 3\n",
      "Inference Time:  2.5975903508010183e-05\n",
      "Testing Accuracy:  97.4\n",
      "\n",
      "Run- 4\n",
      "Inference Time:  2.7317957134003852e-05\n",
      "Testing Accuracy:  97.4\n",
      "\n",
      "Run- 5\n",
      "Inference Time:  3.455353865198269e-05\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "\n",
      "Average Accuracy: 97.404\n",
      "Average Inference Time: 2.9015417691248995e-05\n",
      "Std Inference Time: 3.506364640734996e-06\n"
     ]
    }
   ],
   "source": [
    "# Batch Size 64 experiments\n",
    "all_accuracies_qint8_64, all_inference_qint8_64 = [], []\n",
    "for i in range(5):\n",
    "    print(\"\\nRun-\", (i + 1))\n",
    "    baselineAcc, baselineInf = testLoop(inputModel = model_dynamic_q_int8, dataloader = test_loader_64, batch_size = 64)\n",
    "\n",
    "    all_accuracies_qint8_64.append(baselineAcc)\n",
    "    all_inference_qint8_64.append(baselineInf)\n",
    "    \n",
    "    print(\"Inference Time: \", baselineInf)\n",
    "    print(\"Testing Accuracy: \", baselineAcc)\n",
    "\n",
    "print(\"\\n\\nAverage Accuracy: {}\".format(np.array(all_accuracies_qint8_64).mean()))\n",
    "print(\"Average Inference Time: {}\".format(np.array(all_inference_qint8_64).mean()))\n",
    "print(\"Std Inference Time: {}\".format(np.array(all_inference_qint8_64).std ()))\n",
    "\n",
    "del all_accuracies_qint8_64, all_inference_qint8_64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Quantization (q INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model precision: qint8\n",
      "model:    \t Size (MB): 1.873623\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization.quantize_fx import prepare_fx, prepare_qat_fx, convert_fx\n",
    "qconfig_mapping = torch.ao.quantization.get_default_qconfig_mapping(\"qnnpack\")\n",
    "quantized_model = torch.ao.quantization.quantize_fx.prepare_fx(baseline_model, qconfig_mapping, torch.randn(784, 1000))\n",
    "\n",
    "def calibrate(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input in dataloader:\n",
    "            model(input[0])\n",
    "\n",
    "calibrate(quantized_model, train_loader)\n",
    "static_quantized_model_int8 = torch.ao.quantization.quantize_fx.convert_fx(quantized_model)\n",
    "\n",
    "print(\"Model precision: qint8\")\n",
    "model_size_storgae = print_size_of_model(model_dynamic_q_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run- 1\n",
      "Inference Time:  0.0003821171045303345\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "Run- 2\n",
      "Inference Time:  0.00031534719467163087\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "Run- 3\n",
      "Inference Time:  0.0002933868169784546\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "Run- 4\n",
      "Inference Time:  0.0002848755121231079\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "Run- 5\n",
      "Inference Time:  0.00029399459362030027\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "\n",
      "Average Accuracy: 97.39\n",
      "Average Inference Time: 0.0003139442443847656\n",
      "Std Inference Time: 3.553897779178385e-05\n"
     ]
    }
   ],
   "source": [
    "# Batch Size 1 experiments\n",
    "all_accuracies_qint8_1, all_inference_qint8_1 = [], []\n",
    "for i in range(5):\n",
    "    print(\"\\nRun-\", (i + 1))\n",
    "    baselineAcc, baselineInf = testLoop(inputModel = static_quantized_model_int8, dataloader = test_loader_1, batch_size = 1)\n",
    "\n",
    "    all_accuracies_qint8_1.append(baselineAcc)\n",
    "    all_inference_qint8_1.append(baselineInf)\n",
    "    \n",
    "    print(\"Inference Time: \", baselineInf)\n",
    "    print(\"Testing Accuracy: \", baselineAcc)\n",
    "\n",
    "print(\"\\n\\nAverage Accuracy: {}\".format(np.array(all_accuracies_qint8_1).mean()))\n",
    "print(\"Average Inference Time: {}\".format(np.array(all_inference_qint8_1).mean()))\n",
    "print(\"Std Inference Time: {}\".format(np.array(all_inference_qint8_1).std ()))\n",
    "\n",
    "del all_accuracies_qint8_1, all_inference_qint8_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run- 1\n",
      "Inference Time:  3.7731899387517554e-05\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "Run- 2\n",
      "Inference Time:  2.337014599210897e-05\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "Run- 3\n",
      "Inference Time:  2.4947343738215744e-05\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "Run- 4\n",
      "Inference Time:  2.570341157305772e-05\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "Run- 5\n",
      "Inference Time:  2.0976611383401663e-05\n",
      "Testing Accuracy:  97.39\n",
      "\n",
      "\n",
      "Average Accuracy: 97.39\n",
      "Average Inference Time: 2.654588241486033e-05\n",
      "Std Inference Time: 5.8222700105925355e-06\n"
     ]
    }
   ],
   "source": [
    "# Batch Size 64 experiments\n",
    "all_accuracies_qint8_64, all_inference_qint8_64 = [], []\n",
    "for i in range(5):\n",
    "    print(\"\\nRun-\", (i + 1))\n",
    "    baselineAcc, baselineInf = testLoop(inputModel = static_quantized_model_int8, dataloader = test_loader_64, batch_size = 64)\n",
    "\n",
    "    all_accuracies_qint8_64.append(baselineAcc)\n",
    "    all_inference_qint8_64.append(baselineInf)\n",
    "    \n",
    "    print(\"Inference Time: \", baselineInf)\n",
    "    print(\"Testing Accuracy: \", baselineAcc)\n",
    "\n",
    "print(\"\\n\\nAverage Accuracy: {}\".format(np.array(all_accuracies_qint8_64).mean()))\n",
    "print(\"Average Inference Time: {}\".format(np.array(all_inference_qint8_64).mean()))\n",
    "print(\"Std Inference Time: {}\".format(np.array(all_inference_qint8_64).std ()))\n",
    "\n",
    "del all_accuracies_qint8_64, all_inference_qint8_64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
